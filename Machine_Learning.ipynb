{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyunNi7rz0XlwNckacvHTh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sohrab4u/Data_Tools/blob/main/Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "  - Simple Linear Regression is a method to predict a dependent variable (y) using one independent variable (x) with a straight line equation. It models a linear relationship between them.\n",
        "Example: Predicting house price (y) based on size (x) using y = mx + c."
      ],
      "metadata": {
        "id": "eVlB9kMqR_XS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "  - The key assumptions are:\n",
        "Linear relationship between x and y.\n",
        "Residuals are independent.\n",
        "Constant variance of residuals (homoscedasticity).\n",
        "Residuals are normally distributed. Example: For house price prediction, assume price increases linearly with size."
      ],
      "metadata": {
        "id": "AW9q6_rxSFrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent in the equation y = mx + c?\n",
        "  - The change in y for a one-unit increase in x. In y = 2x + 3, m = 2 means each unit increase in x increases y by 2."
      ],
      "metadata": {
        "id": "3X-mPMFkSQt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation y = mx + c?\n",
        "  - The value of y when x is 0, where the regression line crosses the y-axis. In y = 2x + 3, c = 3 is the y-value when x = 0."
      ],
      "metadata": {
        "id": "cRH3A32ATBgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "  - Calculated as $ m = \\frac{n(\\sum xy) - (\\sum x)(\\sum y)}{n(\\sum x^2) - (\\sum x)^2} $ where n is the number of data points, x and y are variables. Using Python:"
      ],
      "metadata": {
        "id": "nWAT7YV8TG7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "  - Minimizes the sum of squared differences between actual and predicted y values to find the best-fit line. In Python, LinearRegression from sklearn uses this:"
      ],
      "metadata": {
        "id": "R8sItA00TN4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Minimizes the sum of squared differences between actual and predicted y values to find the best-fit line. In Python, LinearRegression from sklearn uses this:\n",
        "  - How is the coefficient of determination (R²) interpreted in Simple Linear Regression?"
      ],
      "metadata": {
        "id": "2dHu4YXoToIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression?\n",
        "  - Predicts y using multiple independent variables (x₁, x₂, ...) with an equation like $ y = b_0 + b_1x_1 + b_2x_2 + \\epsilon $. Predicting house price using size, rooms, and location."
      ],
      "metadata": {
        "id": "wsO4IqO5TyMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "  - Simple Linear Regression uses one predictor, while Multiple Linear Regression uses multiple predictors. Simple: y = 2x + 3; Multiple: y = 2x₁ + 3x₂ + 4."
      ],
      "metadata": {
        "id": "GohjC5jMT4vZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "  - Linear relationship between predictors and y, independence of observations, homoscedasticity, normality of residuals, no multicollinearity. Assume house price depends linearly on size and rooms."
      ],
      "metadata": {
        "id": "2Gb05RNNT90X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "  - Heteroscedasticity is when residual variance changes with x values. It makes coefficient estimates unreliable and affects prediction accuracy. If price prediction errors grow with house size, it’s heteroscedastic."
      ],
      "metadata": {
        "id": "5Ph20-IcUniV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "  - Remove correlated predictors, use PCA, or apply regularization (e.g., Ridge). In Python, use Ridge:"
      ],
      "metadata": {
        "id": "V97LPHfUUstv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "  - Use one-hot encoding or label encoding. In Python:\n",
        "  from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "X_encoded = encoder.fit_transform(data[['location']])"
      ],
      "metadata": {
        "id": "Hj4UkUafUzgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "  - Interaction terms (e.g., x₁x₂) capture combined effects of predictors on y. Size × location might affect house price more than individually."
      ],
      "metadata": {
        "id": "c97GzmcDVAW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "  - In Simple Linear Regression, intercept is y when x=0. In Multiple, it’s y when all x=0, holding other variables constant. In Multiple, intercept is baseline price when size and rooms are 0."
      ],
      "metadata": {
        "id": "leRdJFPAVFvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "  - Slope shows the change in y per unit x. It determines the direction and steepness of the prediction line. Slope = 3 means y increases by 3 per unit x."
      ],
      "metadata": {
        "id": "jghxXnRaVOaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "  - Sets the baseline y value, giving context to the starting point of the relationship. Intercept = 10 in sales prediction sets the base sales when ad spend is 0."
      ],
      "metadata": {
        "id": "ISCEtK5QVUmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "  - Doesn’t indicate model overfitting, doesn’t account for prediction error magnitude, and increases with more predictors. High R² might hide poor out-of-sample predictions."
      ],
      "metadata": {
        "id": "UFEChe8VVafr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "  - Suggests the coefficient is imprecise, indicating uncertainty or multicollinearity. If size coefficient’s standard error is high, its effect on price is unreliable."
      ],
      "metadata": {
        "id": "iui4mFbJVgZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "  - Look for a funnel shape in residual vs. fitted plots. It’s important to address to ensure valid inferences. Use Python:\n",
        "  import matplotlib.pyplot as plt\n",
        "plt.scatter(model.predict(X), y - model.predict(X))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wX4r8-4rVpF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "  - High R² with low adjusted R² suggests unnecessary predictors are inflating R². Adding irrelevant features might raise R² but lower adjusted R²."
      ],
      "metadata": {
        "id": "SBTT9ecsVstt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is important to scale variables in Multiple Linear Regression?\n",
        "  - Ensures all predictors contribute equally, preventing bias toward larger-scale variables. Scale size and price:\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "DdDV8FKGV2Z1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression?\n",
        "  - Fits a curved line using polynomial terms (e.g., $ x^2, x^3 $) to model non-linear relationships. Predicting speed with $ y = 2 + 3x + 0.5x^2 $."
      ],
      "metadata": {
        "id": "Ju--9mpbV753"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression?\n",
        "  - Linear regression uses a straight line; polynomial regression uses a curved line with higher-degree terms. Linear: y = 2x; Polynomial: y = 2x + 0.5x^2."
      ],
      "metadata": {
        "id": "_O90SOUfWHIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used?\n",
        "  - Used when the relationship between x and y is non-linear, like growth curves or exponential trends. Modeling crop yield vs. fertilizer amount."
      ],
      "metadata": {
        "id": "Oh6v4S5lWM4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression?\n",
        "  - $ y = b_0 + b_1x + b_2x^2 + \\dots + b_nx^n + \\epsilon $, where n is the polynomial degree. $ y = 1 + 2x + 3x^2 $."
      ],
      "metadata": {
        "id": "f_qmj5WNWSbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables?\n",
        "  - Yes, by including interaction terms (e.g., $ x_1x_2, x_1^2 $) for multiple predictors. $ y = b_0 + b_1x_1 + b_2x_2 + b_3x_1x_2 $."
      ],
      "metadata": {
        "id": "uNWYiSpTWYXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression?\n",
        "  - Can overfit with high degrees, be hard to interpret, and fail outside data range. A 5th-degree polynomial might fit noise, not the trend."
      ],
      "metadata": {
        "id": "Mz_V_9D1Wejy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "  - Use cross-validation, learning curves, or adjusted R² to choose the best degree. In Python:\n",
        "  from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(model, X, y, cv=5)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "yrcZ3ExXWmPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression?\n",
        "  - Visualization (e.g., scatter plots with fitted curve) helps assess fit and detect non-linear patterns. Plot with matplotlib:\n",
        "  plt.scatter(x, y)\n",
        "plt.plot(x, model.predict(X), color='red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gki3wmuyWtVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in Python?\n",
        "  - Use PolynomialFeatures and LinearRegression from sklearn.\n",
        "  from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(x.reshape(-1, 1))\n",
        "model = LinearRegression().fit(X_poly, y)"
      ],
      "metadata": {
        "id": "tz_BLY2zWy1E"
      }
    }
  ]
}